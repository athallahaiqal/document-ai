# ğŸ“„ document-ai

This is a simple FastAPI application that allows users to:

- âœ… Upload **PDF** or **DOCX** documents
- ğŸ§  Get a **summary** generated by a local **LLM** (via [Ollama](https://ollama.com/))
- â“ Ask **questions** about the content of uploaded documents

The app is fully local â€” no API keys or cloud model usage required.

<!-- TOC -->
* [ğŸ“„ document-ai](#-document-ai)
  * [âš¡ Features](#-features)
  * [ğŸš€ Quick Start](#-quick-start)
    * [1. Install Python 3, uv and ollama](#1-install-python-3-uv-and-ollama)
    * [2. Create a virtual environment with all necessary dependencies](#2-create-a-virtual-environment-with-all-necessary-dependencies)
    * [3. Create a `.env` file at the root of the project](#3-create-a-env-file-at-the-root-of-the-project)
    * [4. Run `llama3.2` locally using Ollama)](#4-run-llama32-locally-using-ollama)
    * [5. Run PostgreSQL and perform migrations`](#5-run-postgresql-and-perform-migrations)
  * [Run application](#run-application)
    * [Development mode](#development-mode)
    * [Production mode](#production-mode)
  * [Linting](#linting)
  * [Formatting](#formatting)
<!-- TOC -->

## âš¡ Features

- ğŸ” **Summarization** of uploaded documents using LLMs (like LLaMA3, Mistral, etc.)
- ğŸ¤– **Context-aware Q&A** on document content
- ğŸ›¡ï¸ Type-safe response models using `pydantic`
- ğŸ“‚ Supports `.pdf` and `.docx` files
- ğŸ”§ Easily swappable LLM backend (via Ollama)

---

## ğŸš€ Quick Start

### 1. Install Python 3, uv and ollama

**MacOS (using `brew`)**

```bash
brew install python@3.13 uv ollama
```

**Ubuntu/Debian**

```bash
sudo add-apt-repository ppa:deadsnakes/ppa
sudo apt update
sudo apt install python3.13
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### 2. Create a virtual environment with all necessary dependencies

From the root of the project execute:

```bash
uv sync
```

### 3. Create a `.env` file at the root of the project

```dotenv
# Ollama
MODEL_NAME=llama3.2

# Database
DATABASE_USER=postgres
DATABASE_PASSWORD=postgres
DATABASE_NAME=postgres
DATABASE_HOST=localhost
DATABASE_PORT=5432
```

### 4. Run `llama3.2` locally using [Ollama](https://ollama.com/))

```bash
ollama run llama3.2
```

### 5. Run PostgreSQL and perform migrations`

```bash
docker compose up -d
alembic upgrade head
```

## Run application

### Development mode

```bash
uv run fastapi dev app/main.py
```

### Production mode

```bash
uv run fastapi run app/main.py
```

## Linting

```bash
ruff check app/* tests/*
```

## Formatting

```bash
ruff format app/* tests/*
```