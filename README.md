# ğŸ“„ document-ai

This is a simple FastAPI application that allows users to:

- âœ… Upload **PDF** or **DOCX** documents
- ğŸ§  Get a **summary** generated by a local **LLM** (via [Ollama](https://ollama.com/))
- â“ Ask **questions** about the content of uploaded documents

The app is fully local â€” no API keys or cloud model usage required.

<!-- TOC -->
* [ğŸ“„ document-ai](#-document-ai)
  * [âš¡ Features](#-features)
  * [ğŸš€ Quick Start](#-quick-start)
    * [1. Install Python 3, uv and ollama](#1-install-python-3-uv-and-ollama)
    * [2. Create a virtual environment with all necessary dependencies](#2-create-a-virtual-environment-with-all-necessary-dependencies)
    * [3. Create a `.env` file at the root of the project](#3-create-a-env-file-at-the-root-of-the-project)
  * [Run application](#run-application)
    * [Development mode](#development-mode)
    * [Production mode](#production-mode)
    * [3. Run ollama locally using `llama3.2`](#3-run-ollama-locally-using-llama32)
  * [ğŸ”Œ API Endpoints](#-api-endpoints)
    * [ğŸ“„ `POST /upload/` â€” **Summarize a document**](#-post-upload--summarize-a-document)
      * [âœ… Request](#-request)
      * [ğŸ¤– Response](#-response)
    * [â“ `POST /ask/` â€” **Ask a question about a document**](#-post-ask--ask-a-question-about-a-document)
      * [ğŸ¤– Response](#-response-1)
<!-- TOC -->

## âš¡ Features

- ğŸ” **Summarization** of uploaded documents using LLMs (like LLaMA3, Mistral, etc.)
- ğŸ¤– **Context-aware Q&A** on document content
- ğŸ›¡ï¸ Type-safe response models using `pydantic`
- ğŸ“‚ Supports `.pdf` and `.docx` files
- ğŸ”§ Easily swappable LLM backend (via Ollama)

---

## ğŸš€ Quick Start

### 1. Install Python 3, uv and ollama

**MacOS (using `brew`)**

```bash
brew install python@3.13 uv ollama
```

**Ubuntu/Debian**

```bash
sudo add-apt-repository ppa:deadsnakes/ppa
sudo apt update
sudo apt install python3.13
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### 2. Create a virtual environment with all necessary dependencies

From the root of the project execute:

```bash
uv sync
```

### 3. Create a `.env` file at the root of the project

```dotenv
# Model name running locally on Ollama
MODEL_NAME=llama3.2
```

## Run application

### Development mode

```bash
uv run fastapi dev app/main.py
```

### Production mode

```bash
uv run fastapi run app/main.py
```

### 3. Run ollama locally using `llama3.2`

```bash
ollama run llama3.2
```

## ğŸ”Œ API Endpoints

### ğŸ“„ `POST /upload/` â€” **Summarize a document**

Uploads a PDF or DOCX file and returns a summarized version of its contents using a local LLM via Ollama.

#### âœ… Request

- **Method:** `POST`
- **URL:** `/upload/`
- **Content-Type:** `multipart/form-data`
- **Form Fields:**
    - `file`: PDF or DOCX file

#### ğŸ¤– Response

```json
{
  "summary": "This document is about..."
}
```

### â“ `POST /ask/` â€” **Ask a question about a document**

Uploads a PDF or DOCX file along with a natural language question. The local LLM will generate an answer based on the
fileâ€™s contents.

âœ… Request

- **Method:** `POST`
- **URL:** `/ask/`
- **Content-Type:** `multipart/form-data`
- **Form Fields:**
    - `file`: PDF or DOCX file
    - `question`: Your question as plain text

#### ğŸ¤– Response

```bash
{
  "answer": "The document describes..."
}
```

ğŸ§ª Example cURL

```bash
curl -X POST http://localhost:8000/ask/ \
  -F "file=@your_file.docx" \
  -F "question=What is the main idea of this document?"
```
